# Sandbox State-Based Tool Pattern

## Overview

The `python_run_prepared` and `file_write` tools now use a **state-based extraction pattern** to handle large code blocks and file contents. This solves serialization issues when LLMs try to pass large strings directly through tool call parameters.

## The Problem

When LLMs generate large code blocks or file contents, passing them directly as tool parameters can cause:
- **Serialization errors** when tool calls are converted to JSON
- **Token limit issues** in tool call parameters
- **Parameter validation failures** with complex content

## The Solution: State-Based Extraction

Instead of passing content directly in tool parameters, content is:
1. **Generated by the LLM** in a markdown code block
2. **Extracted by a custom node** from the AI message content
3. **Stored in graph state** (`pending_content` field)
4. **Retrieved by the tool** when it executes

This pattern separates content generation from tool invocation, avoiding parameter serialization entirely.

---

## Changes to Tools

### `python_run_prepared` (formerly used `pending_code`, now uses `pending_content`)

**Tool Signature:**
```python
async def _arun(
    file_path: str,
    description: str,
    _state: dict,  # Contains pending_content
    tool_call_id: str = "",
    _config: dict | None = None,
    run_manager: AsyncCallbackManagerForToolRun | None = None,
) -> str:
```

**How it works:**
1. LLM generates Python code in a markdown block
2. Custom node extracts code → `state["pending_content"]`
3. Tool retrieves code from `_state["pending_content"]`
4. Tool executes code and returns `Command` to clear state

**Tool parameters (NO CODE PASSED):**
- `file_path`: Where to save the code (e.g., `/tmp/analysis.py`)
- `description`: Brief description of what the code does

### `file_write`

**Tool Signature:**
```python
async def _arun(
    file_path: str,
    description: str,
    _state: dict,  # Contains pending_content
    tool_call_id: str = "",
    _config: dict | None = None,
    run_manager: AsyncCallbackManagerForToolRun | None = None,
) -> str:
```

**How it works:**
1. LLM generates file content in a markdown block
2. Custom node extracts content → `state["pending_content"]`
3. Tool retrieves content from `_state["pending_content"]`
4. Tool writes to VFS and returns `Command` to clear state

**Tool parameters (NO CONTENT PASSED):**
- `file_path`: Where to write the file (e.g., `/data/input.csv`)
- `description`: Brief description of file contents

---

## State Field: `pending_content`

**Single unified field** for both tools:

```python
class AgentState(TypedDict):
    messages: Annotated[list, add_messages]
    pending_content: str  # Used by python_run_prepared and file_write
```

**Why one field?**
- Both tools extract content from markdown blocks using identical logic
- Simpler state management (no need to track separate fields)
- TypedDict requires all fields to be initialized; one field is easier

---

## LangGraph Agent Integration

### Required Components

#### 1. **AgentState Definition**

```python
from typing_extensions import TypedDict
from typing import Annotated
from langgraph.graph.message import add_messages

class AgentState(TypedDict):
    """State matching maistack usage."""
    messages: Annotated[list, add_messages]
    pending_content: str  # Content extracted from AI message
```

#### 2. **Content Extraction Map**

Maps tool names to their state field:

```python
content_extraction_map = {
    "python_run_prepared": "pending_content",
    "file_write": "pending_content",
}
```

#### 3. **Custom Tool Node**

Replaces LangGraph's default `ToolNode` to extract content before tool execution:

```python
import re
from langchain_core.messages import AIMessage, ToolMessage
from langgraph.types import Command

async def custom_tool_node(state: AgentState, config: RunnableConfig) -> dict:
    """Custom tool node that extracts content before calling tools."""
    messages = state.get("messages", [])
    last_message = messages[-1]

    if not hasattr(last_message, "tool_calls") or not last_message.tool_calls:
        return {"messages": []}

    tool_state = {}
    msgs = []

    # Extract content from AI message for tools that need it
    tool_names = [tc["name"] for tc in last_message.tool_calls]
    for tool_name in tool_names:
        if tool_name in content_extraction_map:
            state_field = content_extraction_map[tool_name]

            if isinstance(last_message, AIMessage) and last_message.content:
                content = last_message.content

                # Handle list of content blocks
                if isinstance(content, list):
                    text_parts = []
                    for block in content:
                        if isinstance(block, dict) and block.get("type") == "text":
                            text_parts.append(block.get("text", ""))
                        elif isinstance(block, str):
                            text_parts.append(block)
                    content = "\n".join(text_parts)

                # Extract content from markdown block
                # Matches: ```python\ncode\n```, ```csv\ndata\n```, etc.
                content_match = re.search(r"```(?:\w+)?\n(.*?)\n```", content, re.DOTALL)

                if content_match:
                    extracted_content = content_match.group(1)
                    tool_state[state_field] = extracted_content

    # Execute tool calls
    for tool_call in last_message.tool_calls:
        tool_name = tool_call["name"]
        tool = tools_by_name.get(tool_name)

        if tool is None:
            msgs.append(
                ToolMessage(
                    f"Unknown tool: {tool_name}",
                    tool_call_id=tool_call.get("id", "unknown"),
                )
            )
            continue

        try:
            kwargs = tool_call.get("args", {})

            # Inject state for content-based tools
            if tool_name in content_extraction_map:
                state_field = content_extraction_map[tool_name]
                # Only pass the relevant state field (avoids serialization issues)
                serializable_state = {
                    state_field: state.get(state_field, "") or tool_state.get(state_field, ""),
                }
                kwargs["_state"] = serializable_state
                kwargs["tool_call_id"] = tool_call.get("id", "")
                # NOTE: Do NOT pass _config here - causes serialization errors

            # Call tool
            if hasattr(tool, "ainvoke"):
                result = await tool.ainvoke(kwargs)
            else:
                result = tool.invoke(kwargs)

            # Handle Command return type (state update from tool)
            if isinstance(result, Command):
                # Tool returned state updates (e.g., clearing pending_content)
                update_dict = result.update
                for key, value in update_dict.items():
                    if key != "messages":
                        tool_state[key] = value

                # Extract the actual result string
                result_str = result.resume if result.resume else "Tool executed successfully"
                msgs.append(
                    ToolMessage(result_str, tool_call_id=tool_call.get("id", ""))
                )
            else:
                msgs.append(ToolMessage(result, tool_call_id=tool_call.get("id", "")))

        except Exception as e:
            msgs.append(
                ToolMessage(
                    f"Error executing {tool_name}: {e}",
                    tool_call_id=tool_call.get("id", ""),
                )
            )

    # Update state with extracted content and messages
    update = {"messages": msgs}
    if tool_state.get("pending_content") is not None:
        update["pending_content"] = tool_state["pending_content"]

    return update
```

#### 4. **Graph Construction**

```python
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

def create_agent_graph(db_pool, include_tools=None):
    """Create LangGraph agent with state-based content extraction."""

    # Create tools
    tools = create_sandbox_tools(
        db_pool,
        thread_id="your_thread_id",
        include_tools=include_tools or ["python_run_prepared", "file_write"],
    )

    # Create tools mapping for custom node
    tools_by_name = {tool.name: tool for tool in tools}

    # Create LLM with tools
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    llm_with_tools = llm.bind_tools(tools)

    def agent_node(state: AgentState) -> dict:
        """Agent node that calls LLM."""
        messages = state.get("messages", [])
        response = llm_with_tools.invoke(messages)
        return {"messages": [response]}

    def should_continue(state: AgentState) -> str:
        """Decide whether to continue or end."""
        messages = state.get("messages", [])
        last_message = messages[-1]

        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            return "tools"
        return "end"

    # Build graph
    workflow = StateGraph(AgentState)
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", custom_tool_node)  # Use custom node

    workflow.set_entry_point("agent")
    workflow.add_conditional_edges("agent", should_continue, {"tools": "tools", "end": END})
    workflow.add_edge("tools", "agent")

    return workflow.compile(checkpointer=MemorySaver())
```

---

## Usage Examples

### Example 1: Python Code Execution

**User prompt:**
```
"Write Python code to calculate the first 10 Fibonacci numbers and print them. Use python_run_prepared to execute it."
```

**LLM Response:**
```
I'll create Python code to calculate Fibonacci numbers.

```python
def fib(n):
    if n <= 1:
        return n
    return fib(n-1) + fib(n-2)

for i in range(10):
    print(f"F({i}) = {fib(i)}")
```
```

**Tool Call:**
```json
{
  "name": "python_run_prepared",
  "arguments": {
    "file_path": "/tmp/fibonacci.py",
    "description": "Calculate and print first 10 Fibonacci numbers"
  }
}
```

**What happens:**
1. Custom node extracts code from markdown block
2. Stores in `state["pending_content"]`
3. Tool retrieves code from state
4. Tool executes code
5. Tool returns `Command` to clear `pending_content`

### Example 2: File Write

**User prompt:**
```
"Create a CSV file with sample employee data at /data/employees.csv"
```

**LLM Response:**
```
I'll create a CSV file with employee data.

```csv
name,age,department,salary
Alice Johnson,32,Engineering,95000
Bob Smith,28,Marketing,72000
Carol Williams,45,Management,120000
David Brown,35,Engineering,88000
```
```

**Tool Call:**
```json
{
  "name": "file_write",
  "arguments": {
    "file_path": "/data/employees.csv",
    "description": "Employee database with 4 records"
  }
}
```

**What happens:**
1. Custom node extracts CSV from markdown block
2. Stores in `state["pending_content"]`
3. Tool retrieves content from state
4. Tool writes to VFS
5. Tool returns `Command` to clear `pending_content`

---

## Migration Guide

### If you have existing agents using old file_write:

**Old pattern (BROKEN):**
```python
# LLM tried to pass content in tool parameters
{
  "name": "file_write",
  "arguments": {
    "file_path": "/data/data.csv",
    "content": "very large CSV content..."  # ❌ Causes serialization errors
  }
}
```

**New pattern (WORKS):**
```python
# 1. LLM generates content in markdown block (in message.content)
# 2. Custom node extracts → state["pending_content"]
# 3. LLM makes tool call with NO content
{
  "name": "file_write",
  "arguments": {
    "file_path": "/data/data.csv",
    "description": "Large CSV dataset"  # ✅ Only metadata
  }
}
```

### Steps to migrate:

1. **Update AgentState** to include `pending_content` field
2. **Create content extraction map** for your tools
3. **Replace ToolNode** with custom_tool_node implementation
4. **Initialize pending_content** in all agent invocations:
   ```python
   result = await app.ainvoke(
       {
           "messages": [...],
           "pending_content": "",  # ✅ Required
       },
       config={"configurable": {"thread_id": "..."}},
   )
   ```

---

## Key Implementation Details

### ⚠️ Important: Do NOT pass `_config` to tools

```python
# ❌ WRONG - causes "Runtime is not JSON serializable" error
kwargs["_config"] = config

# ✅ CORRECT - tools extract thread_id from run_manager
# (already handled in tool implementation)
```

### Command Pattern for State Updates

Tools return `Command` objects to update state:

```python
from langgraph.types import Command
from langchain_core.messages import ToolMessage

state_update = {
    "pending_content": "",  # Clear after use
    "created_files": [file_path],
    "messages": [ToolMessage(content=result, tool_call_id=tool_call_id)],
}

return Command(update=state_update, resume=result)
```

### Markdown Extraction Regex

Matches code blocks with optional language specifier:

```python
# Matches: ```python\ncode\n```, ```csv\ndata\n```, ```\ncode\n```
content_match = re.search(r"```(?:\w+)?\n(.*?)\n```", content, re.DOTALL)
```

---

## Testing

### Unit Tests

Both tools have comprehensive unit tests:

```bash
pytest tests/test_execute_code.py -v  # 5 tests
pytest tests/test_file_write.py -v    # 4 tests
```

### E2E Tests

End-to-end tests with real LLM:

```bash
pytest tests/test_python_run_prepared_e2e.py -v
```

Tests verify:
- Content extraction from markdown blocks
- State updates and clearing
- File creation and persistence
- Error handling

---

## Benefits

✅ **No serialization errors** - Content never goes through tool parameters
✅ **Handles large content** - No token limits on tool parameters
✅ **Cleaner tool signatures** - Tools only need metadata (file paths, descriptions)
✅ **Unified pattern** - Same approach for both python_run_prepared and file_write
✅ **State management** - Content automatically cleared after execution
✅ **LangGraph compatible** - Uses Command pattern for state updates

---

## Troubleshooting

### "No content found in graph state"

**Cause:** `pending_content` not extracted or initialized

**Fix:**
1. Verify custom_tool_node is extracting content correctly
2. Check LLM generated content in markdown block
3. Ensure state initialized with `"pending_content": ""`

### "Object of type Runtime is not JSON serializable"

**Cause:** Passing `_config` parameter to tools

**Fix:** Remove `kwargs["_config"] = config` from custom_tool_node

### "Recursion limit reached"

**Cause:** LLM not following instructions to use tools (LLM behavior issue)

**Fix:**
1. Improve system prompt clarity
2. Increase recursion limit in config
3. Add explicit tool usage examples

---

## References

- **Tool implementations:** `src/mayflower_sandbox/tools/execute_code.py`, `src/mayflower_sandbox/tools/file_write.py`
- **E2E test example:** `tests/test_python_run_prepared_e2e.py`
- **Unit tests:** `tests/test_execute_code.py`, `tests/test_file_write.py`
